{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice 11 - *Ensembles intro*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Simple tf-idf + Random forest routine for sentiment analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kolos Maria BSE-141"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Reading the training data in order to obtain a list of texts and numpy.array of scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('data/train_feedback.txt')\n",
    "data=file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts=[]\n",
    "scores=[]\n",
    "for line in data:\n",
    "    raw=eval(line)\n",
    "    texts.append(raw['text'])\n",
    "    scores.append(raw['score'])\n",
    "scores=np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Obtaining the tf-idf representations via TfidfVectorizer from the sklearn library. Tuning min_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.cross_validation import train_test_split#using python 3.5 and scipy 0.17\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_size=50000\n",
    "N=len(texts)\n",
    "\n",
    "def get_samples(min_df):\n",
    "    vect= TfidfVectorizer(sublinear_tf=True, min_df=min_df, analyzer='word',  stop_words='english')\n",
    "    matrix=vect.fit_transform(texts)\n",
    "    train_X, test_X,train_y,test_y=train_test_split(matrix,scores, train_size=train_size/N)\n",
    "    return train_X, test_X,train_y,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#choosing optimal min_df on default RandomForestRegressor due to computation speed issue\n",
    "matrix_len=[]\n",
    "mse_score=[]\n",
    "frequency=np.arange(0.001, 0.005,0.001)\n",
    "\n",
    "\n",
    "for freq in frequency:\n",
    "    train_X, test_X,train_y,test_y=get_samples(freq)\n",
    "    rfr=RandomForestRegressor(random_state = 42)\n",
    "    rfr.fit(train_X,train_y)\n",
    "    res=((mean_squared_error(test_y, rfr.predict(test_X)),freq))\n",
    "    print(res,matrix.shape[1])\n",
    "    mse_score.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Randomly splitting the data into training (50000 items) and validation sets. Tuining the model with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
       "           verbose=0, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=1,\n",
       "          param_distributions={'max_features': array([ 0.01,  0.02,  0.03,  0.04,  0.05,  0.06,  0.07,  0.08,  0.09,  0.1 ]), 'min_samples_leaf': [2, 3, 4]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring=None, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "param_dist = {\"max_features\": np.linspace(0.01,0.1,10),\n",
    "              \"min_samples_leaf\": [2,3,4]}\n",
    "\n",
    "train_X, test_X,train_y,test_y=train_X, test_X,train_y,test_y=get_samples(0.001)\n",
    "\n",
    "model = RandomForestRegressor(random_state =42)\n",
    "n_iter_search = 10\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,n_iter=n_iter_search)\n",
    "\n",
    "random_search.fit(train_X,train_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking up the best parameters (for MSE criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 0.070000000000000007, 'min_samples_leaf': 2}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the final model with the best parameters and  evaluating the MSE error of the obtained regressor compared to the trivial baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X, test_X,train_y,test_y=get_samples(0.001)\n",
    "model = RandomForestRegressor(n_estimators=100,random_state =42,max_features=0.07)\n",
    "model.fit(train_X,train_y)\n",
    "print(mean_squared_error(test_y, model.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.43694585811\n"
     ]
    }
   ],
   "source": [
    "#trival baseline\n",
    "mean_score=np.mean(train_y)\n",
    "dummy_predict_y=[mean_score]*len(test_y)\n",
    "\n",
    "print(mean_squared_error(test_y,dummy_predict_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSE score reduced nearly twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Checking the interpretability of the obtained regressor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = model.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "vocabulary =vect.get_feature_names()\n",
    "\n",
    "print(\"10 most important features\")\n",
    "\n",
    "for f in range(10):\n",
    "    print((vocabulary[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They seem to be reasonable and they all have strong negative context which is likely to affect the score unambiguously (down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
